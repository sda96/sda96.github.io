---
title: NLP Paper (11) - RoBERTa
categories: [NLP]
comments: true
use_math: true
---



# [RoBERTa] A Robustly Optimized BERT Pretraining Approach



[**[RoBERTa] 강건하게 최적화된 BERT를 사전학습시키는 접근법 **](https://arxiv.org/abs/1907.11692)



해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.



## Abstract

- 해당 논문은 기존의 BERT 논문의 파생 연구로써 **BERT는 훈련이 부족한 상태**로 사전학습 시키는데 **주요한 하이퍼파라미터들**과 훈련시키는 **데이터의 크기**와 같은 요인들을 실험하였습니다.



### Introduction

- 다양한 자기 지도 학습 방법들이 나오고 있는데 이 방법들에서 성능을 개선 시키는 부분을 결정하기 어렵습니다.
- 어려운 이유는 사전학습시 계산량이 많고 제한된 양의 파라미터를 튜닝하기 때문입니다.
- 해당 논문에서 제시하는 성능을 개선시키는 BERT 학습방법은 다음과 같습니다.
  - 학습을 길게, 배치를 크게, 데이터를 많이 넣습니다.
  - BERT에서 Next Sentence Prediction (NSP) 부분을 제거합니다.
  - 더욱 긴 시퀀스 데이터를 학습시킵니다.
  - 기존의 BERT 보다 더욱 다이나믹한 마스킹 패턴을 훈련데이터에 적용합니다.
- 모델의 성능을 비교하기 위한 새로운 데이터셋 (CC-NEWS) 도 만들어서 모델을 검증해보았습니다.
- 요약하자면 해당 논문에서 말하고자 하는 내용은 다음과 같습니다.
  - 연구진은 BERT 디자인 선택의 중요성과 훈련 시키는 전략 그리고 소개하는 대안책들은 downstream task 에서의 더 나은 성능을 만들어 줄 것 입니다.
  - CC-NEWS 데이터셋으로 모델을 검증할 수 있습니다.
  - 연구진이 만든 마스크 언어 모델의 사전학습 방식이 남들보다 좋은 성능을 냈기에 올바른 디자인 선택이라 볼 수 있습니다. 
