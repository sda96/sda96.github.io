---
title: NLP Paper (5) - Seq2Seq
categories: [NLP]
comments: true
use_math: true
---



# [Seq2Seq] Sequence to Sequence Learning with Neural Networks



해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.

 

## Abstract

- DNN을 사용하여 end-to-end 방식의 시퀀스 학습을 할 수 있는 시퀀스 구조를 제안합니다.
- 한 쪽에서는 다층의 LSTM 모델을 사용하여 입력 시퀀스와 고정된 차원의 하나의 벡터로 맵핑을 시키고, 또 다른 LSTM 모델은 앞의 벡터로부터 출력 시퀀스를 디코드(복호화)시킵니다.
- 데이터셋은 영어-프랑스어 번역 태스크인 WMT 14 데이터셋을 사용하여 BLEU score 34.8을 성취했고 추가적으로 36.5까지 성능을 올렸습니다.
- 해당 모델은 기존의 SMT 모델에 비해서 긴 문장에 대한 학습도 잘되었고, 구절과 문장의 표현을 민감하게 학습하였습니다.



### 1. Introduction

- DNN은 강력한 도구이지만 입력들과 출력들이 고정된 차원의 벡터들로 인코딩되어지는 경우에만 적용되어지고 있습니다.

- 하지만 고정된 차원의 벡터로만 인코딩되는 경우 대부분의 문제들이 고정되지 않은 시퀀스이기에 한계에 부딪치게 됩니다.

- 그러므로 시퀀스를 시퀀스로 맵핑하는 것을 학습하는 것이 유용할 방법일 것 입니다. 

- 연구진이 제안하는 방법은 첫 LSTM 모델은 입력 시퀀스의 매 시점을 읽는데 사용하여 고정된 차원의 벡터 표현을 획득하고, 또 다른 LSTM 모델은 앞선 LSTM에서 얻은 벡터를 사용하여 출력 시퀀스를 추출합니다.

  - 두 번쨰 LSTM 모델은 재귀적 신경망 네트워크 기반의 언어모델로 입력 시퀀스를 제외한다는 조건을 둡니다.

- ![image](https://user-images.githubusercontent.com/51338268/146680102-99a72775-21ed-4852-b8d0-3389a3cc45b1.png)

  입력 문장으로 "ABC"가 들어오고 출력 문장으로써 "WXYZ"를 생성합니다. 모델은 \<EOS> 토큰이 나오면 예측을 만드는 것을 멈춥니다. LSTM은 입력 문장을 반대로 읽습니다, 왜냐하면 그렇게 함으로써 문제를 더 쉽게 최적화 시켜주는 데이터를 사용한 많은 단기 기억 의존성이 도입되어집니다.

- 다양한 길이의 입력 문장과 고정된 차원의 벡터 표현을 맵핑시키는 LSTM 모델의 특성은 입력 문장을 요약하는 경향이 있다는 것 입니다.

- 정성적으로 평가하였을 때, 연구진의 모델은 단어의 순서 인지하고 수동태와 능동태의 공정한 불변성을 가진다고 볼 수 있었습니다.



### 2. The model

- RNN도 훌륭한 시퀀스 학습 모델이지만 장기 기억 문제 떄문에 학습시키기 어려워서 LSTM을 사용하는 것이 더 좋습니다.
- 연구진이 제안하는 모델 구조는 다음과 같습니다.
  - $p(y_1, \cdots, y_{T'} \vert x_1, \cdots x_{T}) = \prod^{T'}_{t=1} p(y_t \lvert v, y_1, \cdots , y_{t-1})$​
  - $v$는 입력 문장과 맵핑된 고정 차원의 표현 벡터입니다.
- 앞의 표현에서 실제 모델과 다른점이 3가지 있습니다.
  - 실제 모델에서는 서로 다른 LSTM 모델을 2개를 사용하였습니다.
  - 얕은 모델 LSTM보다 깊은 모델이 더 성능이 좋았으며 층을 4개 쌓아서 사용하였습니다.
  - 입력 문장의 순서를 반대로 바꾸어서 사용하였더니 더 성능이 좋았습니다. (왜 성능이 더 좋았는지 우리도 몰라요..)

