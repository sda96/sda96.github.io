---
title: NLP Paper (04) - Bahdanau Attention
categories: [NLP]
comments: true
use_math: true
---



# [Bahdanau Attention] NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE



해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.

 

## Abstract

- 연구진은 기존의 인코더-디코더 구조에서 성능을 향상시키는데 병목현상을 일으키는 원인이 고정된 길이의 벡터를 사용하기 때문이라 생각했습니다.
- 연구진은 **예측하려는 하나의 타겟 단어와 관련있는 부분들을 입력 문장에서 (자동적으로 소프트하게) 탐색하는 모델**을 제안하였습니다.
- 해당 방법은 영어-프랑스어 번역 태스크에서 존재하는 SOTA와 비교할만한 성능을 얻었습니다.
- 또한, 정성적인 분석을 한 결과 해당 모델이 탐색한 것들이 사람의 직관과도 잘 맞다는 것을 발견했습니다.



### 1. Introduction

#### 1.1 신경망 기계번역에서 사용되는 인코더-디코더 구조

- 기계번역 분야에서 최근 신경망 기계번역이 떠오르고 있으며 모델의 구조는 일반적으로 인코더-디코더 구조입니다.
- 해당 구조는 인코더가 입력 문장을 읽고 고정 길이 벡터로 인코딩하며 디코더는 인코딩된 벡터를 받아서 번역된 결과물을 출력합니다.
- 인코더-디코더 구조는 한 쌍의 언어를 다루며 입력 문장이 주어졌을 때, 올바른 번역을 반환시킬 확률을 높이기 위해서 공동으로(인코더와 디코더가 동시에) 학습되어집니다.



#### 1.2 기존 인코더 디코더 문제점

- 인코더-디코더 구조의 인코더는 입력 문장에 있는 필요한 정보들을 모두 압축시키기에 긴 문장일수록 모델의 성능이 급격히 나빠지는 문제가 발생합니다.



#### 1.3 문제 해결 방법 제시

- 이러한 문제를 해결하기 위해서 해당 논문에서는 새로운 접근법을 제안합니다.
- 각각의 시점에 모델이 하나의 단어를 생성할 때, 입력 문장에서 (생성된 단어와) 가장 연관된 정보를 가진 위치들의 집합을 찾는데 집중합니다.

- 모델이 하나의 타겟 단어를 예측할 때, **입력 문장의 위치값과 관련된 문장벡터들**과 **이전에 생성되어진 모든 타겟 단어들**을 기반으로 예측합니다.
- 해당 논문의 접근법이 기존의 인코더 디코더 모델과 다른 특징을 가집니다.
  - 기존 방법은 고정 길이 벡터로 인코딩 되었지만 논문의 접근법은 벡터들을 하나의 시퀀스로 인코딩합니다.
  - 번역으로 디코딩되는 동안 적응되는(학습되는) 벡터들의 부분집합을 선택합니다.
- 해당 접근법으로 고정 길이 벡터임에도 불구하고 입력 문장의 정보를 압착할 수 있게 되었고 긴 문장에서도 성능이 괜찮았습니다.



### 2. BACKGROUND: NEURAL MACHINE TRANSLATION

- 일반적인 기계번역 모델은 입력 문장 $\bold x$​가 주어졌을 때, 출력 문장 $\bold y$인 조건부 확률이 가장 크게되는 출력 문장 $y$를 구하는 것이 기본 구조입니다.
  - $\arg max_y p(\bold y \lvert \bold x)$​

- 신경망 기계 번역의 경우 입력문장을 인코딩 시키는 인코더와 인코딩된 입력 문장을 받아서 출력 문장을 생성하는 디코더의 구조로 되어있습니다.
  - 인코더, 디코더의 구조이기에 필요한 모델의 개수도 2개입니다.
  - 모델로 RNN모델을 사용되어질 수 있습니다.

- 신경망의 번역 시스템의 방식으로 2가지가 있습니다.
  - 한 쌍의 문장들로 이루어진 문장 테이블에 점수를 매기는 방식
  - 후보 번역본들의 순위를 재지정하는 방식



### 3. LEARNING TO ALIGN AND TRANSLATE

- 인코더 부분은 양방향 RNN 모델을 사용하였으며 디코더 부분은 다음과 같습니다.

#### 3.1 디코더

![image](https://user-images.githubusercontent.com/51338268/146635782-e4b40927-0d1c-47b5-a893-b94834e193b3.png)

- $p(y_i \lvert y_1 \cdots, y_{i-1} , \bold x) = g(y_{i-1}, s_i, c_i)$
  - $y_i$는 $i$시점의 출력값
  - $\bold x$는 입력 문장 벡터로 $x_1, x_2, \cdots x_T$ 는 $T$시점의 단어들입니다.
  - $s_i$는 $i$시점의 RNN 모델의 hidden state
  - $c_i$는 $i$시점의 context vector로 context vector란 **인코더에서 가져온 hidden state**라고 보면 됩니다.
- $s_i = f(s_{i-1}, y_{i-1}, c_i)$
- $c_i = \sum^{T_x}_{j=1} \alpha_{ij}h_j$​
  - $c_i$는 $h_j$의 가중합입니다.
  - $\alpha_{ij} = \frac{exp(e_{ij})}{\sum^{T_{x}}_{k=1}exp(e_{ik})}$​로 소프트맥스를 하기에 $\alpha_{ij}$​는 0과 1사이의 확률이기에 점수로 볼 수 있습니다.
  - $e_{ij} = a(s_{i-1}, h_j)$​를 alignment model이라고 부릅니다.
    - $a$​는 학습이 가능한 파라미터입니다.
    - $e_{ij}$는 입력층의 hidden state인 $h_j$의 중요도를 반영합니다.



#### 3.2 인코더

인코더부분은 BiRNN 구조를 사용하였습니다.



### 4. EXPERIMENT SETTINGS

연구진은 ACL WMT 14 데이터셋에 있는 영어-프랑스어 번역 태스크로 성능을 실험한 결과 다음과 같습니다.

![image](https://user-images.githubusercontent.com/51338268/146674736-5cdfbc84-0816-4c71-82b5-c48c8772e1a9.png)

BLEU score 지표를 기준으로 기존 인코더 디코더 모델과 연구진의 개선된 인코더 디코더 모델의 성능을 문장의 길이에 따라서 비교하였습니다.

결과는 \<unk>이 포함된 문장의 길이가 길어질수록 연구진의 개선된 인코더 디코더 모델의 성능은 안정적으로 유지되는 것을 알 수가 있었습니다.
