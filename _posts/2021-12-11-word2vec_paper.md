---
title: NLP Paper (1) - Word2vec
categories: [NLP]
comments: true
use_math: true
---



# [Word2vec] Efficient Estimation of Word Representations in Vector Space



해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.

 

## Abstract

막대한 양의 데이터셋에 있는 단어들의 연속적인 벡터 표현을 계산하기 위해서 2가지 모델 구조를 제안합니다. 

표현의 품질은 단어 유사도 태스크를 통해서 측정하고, 결과들은 과거의 가장 좋은 성능을 냈던 기술들을 서로 다른 종류의 신경망 네트워크를 사용하여 비교해보겠습니다. 

연구진은 엄청난 정확도의 개선만큼 덜 소모하는 컴퓨터 비용을 관측하였습니다. 

예를 들어, 16억개의 단어로 이루어진 데이터셋으로부터 높은 품질의 단어 벡터를 학습시키는데 하루도 안걸렸습니다. 게다가, 이러한 단어 벡터들은 단어의 구문과 의미 유사도를 측정하는 테스트셋에서 SOTA 수준의 성능을 보여주었습니다.

#### 3줄 요약

- 막대한 양의 데이터셋을 사용하여 단어의 연속적인 벡터 표현을 학습할 구조 2가지를 소개합니다.
- 단어의 벡터 표현에 대한 성능은 단어 유사도 태스크와 각종 신경망 모델을 사용하여 비교해보겠습니다.
- 이 방법은 성능도 좋았고 컴퓨터 자원도 덜 소모했습니다.



## 1. Introduction

오늘날 많은 NLP 시스템과 기술들은 단어들을 원자(문자) 단위로 다룹니다 - 이 방법은 단어들간의 유사성이 존재하지 않으며, 이러한 단어들은 단어장의 인덱스로 표현되어집니다.

이러한 선택을 하는 여러가지 좋은 이유가 있습니다 - 간편함, 강건함하며, 막대한 양의 데이터를 학습한 간단한 모델이 더 적은 데이터로 학습한 복잡한 시스템의 모델보다 더 좋은 성능을 냅니다.

예를 들어서 유명한 N-gram 모델이 존재하며 이 모델은 통계적 언어 모델로써 사용되어집니다.

N-gram 모델은 이론적으로 모든 데이터들을 학습할 수 있습니다. (수 조 개의 단어들)



하지만 간단한 기술들은 많은 태스크에서 그들의 한계를 맞이하게 됩니다.

예를 들어 자동 발성 인식과 같은 특정 도메인 데이터의 경우 한계가 있습니다 - 성능은 일반적으로 고품질의 변환된 발성 데이터의 크기에 따라서 영향을 받습니다. (종종 수백만개의 단어들인 경우)

기계번역 태스크의 경우, 많은 언어에서 존재하는 말뭉치가 종종 수백만개 혹은 그보다 적은 단어만 포함하고 있습니다. 그래서 기본적인 기술을 통하여 간단하게 용량을 늘리는 상황은 과정에 치명적인 결과를 만들지 않을 것입니다. 그리고 연구진은 더욱 진보한 기술들에 집중해야만 합니다.



최근 몇년간의 머신 러닝 기술들의 진보와 함께, 더욱 복잡한 모델들은 더욱 큰 데이터셋으로 학습시키는 것이 가능해졌습니다.

그리고 모델들은 일반적으로 간단한 모델들의 성능을 능가했습니다.

아마도 가장 성공적인 개념은 단어의 분산표현을 사용한 것입니다.

예를 들어, 뉴럴 네트워크 기반의 언어모델은 N-gram 모델들의 성능을 훨씬 뛰어넘습니다.

#### 3줄 요약

- 단어들을 인덱스로 표현하여도 간단한 N-gram 모델에서 성능을 냈습니다.
- 하지만 데이터의 부족, 특정 도메인 데이터의 경우 성능의 한계를 맞이합니다.
- 최근 들어 모델의 성능을 이끌어준 개념은 **단어의 분산표현**입니다.



### 1.1 Goals of the Paper

해당 논문의 주요 목표는 높은 수준의 단어 벡터를 수십억개의 단어들과 단어장에 있는 수백만개의 단어들이 담긴 막대한 데이터셋 사용하여 학습시키는 기술들을 소개하기 위함입니다.

우리가 아는 한, 이전에 제안된 구조들중에서는 수백만개 이상의 단어들을 50에서 100 차원의 단어벡터들로 성공적으로 학습된 적은 없었습니다.



연구진은 결과적인 벡터의 표현의 품질을 측정하기 위해서, 비슷한 단어끼리는 서로 가까이 있는 경향이 있다고 기대될 뿐만 아니라, 단어들이 다양한 각도의 유사성을 가지고 있다라는 최근에 제안된 기술들 사용하였습니다.

이 현상은 일찍히 굴절어 문장에서 관측되어져 왔습니다.

예를 들어, 명사들은 다양한 어미를 가질 수 있습니다, 만약 우리가 유사한 단어들을 벡터 공간의 부분 공간에서 찾게 된다면, 비슷한 어미를 가진 단어들을 찾는 것이 가능합니다.

꽤 놀랍게도, 단어 표현의 유사성은 간단한 의미적 규칙성을 파악한 것을 발견하였습니다. 단어 오프셋 기술을 사용하여 단어 벡터들의 간단한 선형연산이 가능합니다, 예시로 들자면 $vector('King') - vector('Man') + vector('Woman')$의 결과는 벡터 표현에서 가장 가까운 단어인 $Queen$를 반환하게 됩니다.



해당 논문에서는 이러한 벡터 연산을 단어간의 선형적 규칙성을 보존하는 새로운 모델 구조를 개발함으로써 정확도를 최대가 되도록 노력할 것 입니다.

연구진은 구문과 의미적 규칙성을 동시에 측정하기 위해서 새롭게 이해 가능한 테스트셋을 설계하였습니다, 그리고 많은 규칙성들을 높은 정확도로 학습할 수 있었다는 것을 보여줍니다.

게다가, 우리는 학습 시간과 정확도가 단어 벡터들의 차원과 학습 데이터의 양에 얼마나 의존적인지 토의하였습니다.

#### 3줄 요약

- 구문적, 의미적 규칙성을 학습한 고품질의 단어 벡터를 50 ~ 100차원으로 만들 수 있는 모델 구조를 소개하려고 합니다.
- 단어간의 유사성은 의미적 규칙성을 파악할 수 있었고 학습된 단어 벡터들은 선형 연산의 정확도가 최대가 되도록 만들었습니다.
- 구문과 의미적 규칙성을 측정하기 위해서 새로운 데이터셋을 만들었고 단어 벡터의 차원 크기, 학습 데이터양이 끼치는 영향을 실험하였습니다.



### 1.2 Previous Work

단어를 연속적인 벡터로 표현하는 것은 오랜 역사를 가지고 있습니다.

추정을 위한 가장 인기있는 모델 구조는 신경망 네트워크 언어 모델(NNLM)이 제안되었습니다. 

선형 사영 층과 비선형 은닉 층이 함께 있는 순방향 신경망 네트워크는 통계적 언어 모델을 공동으로 단어 벡터 표현을 학습시키기 위해서 사용되어 졌습니다.

이 작업은 많은 이들이 지지해오고 있었습니다.



NNLM의 또다른 흥미로운 구조는 [13, 14]에서 제안되어졌습니다. 이 방법은 단어 벡터들이 처음에 은닉층이 하나인 신경망 네트워크를 사용하여 학습되어 집니다.

단어 벡터들은 NNLM을 학습하기 위해서 사용되어집니다. 그리고 단어 벡터들은 심지어 완전한 상태의 NNLM을 구축없이도 학습되어집니다.

이 작업에서는 연구진은 이 구조를 확장하였으며 그리고 단지 단어 벡터가 간단한 모델을 사용하여 학습되어지는 첫 번째 단계에 집중하였습니다.



추후에는 단어 벡터들은 의미 있는 성능의 개선과 간단한 NLP 어플리케이션에 사용되어졌습니다.[4,5,29]

단어 벡터 자체를 추정하는 것은 다른 모델 구조를 사용하여 성능이 좋아졌고 다양한 말뭉치에서 학습되어졌으며[4,29,23,19,9] 그리고 단어 벡터들의 몇가지 결과들은 미래의 연구와 비교를 하는데 이용가능하도록 만들었습니다.

하지만, 우리가 아는한, 앞의 논문들에 대한 구조는 학습을 하는데 큰 컴퓨터적 자원이 크게 소모되게 됩니다. 특정한 버전의 모델들은 제외하고 말입니다.

#### 3줄 요약

- 이전의 논문에서는 NNLM을 통하여 단어 벡터들을 학습하였습니다.
- 하지만 해당 논문에서는 단 하나의 은닉층을 사용한 신경망 네트워크를 사용하여 단어 벡터들을 학습시킵니다.



## 2. Model Architectures

LSA, LDA를 포함하여, 연속형의 단어 표현을 추정하기 위해서 많은 종류의 모델들이 제안되어졌습니다.

해당 논문에서는 신경망 네트워크를 통하여 단어들의 분산표현을 학습하는 것에 집중합니다.

이전의 내용을 보았다시피 이 방법은 단어간의 선형 규칙성을 보존하기 위한 LSA보다 월등한 성능을 보여주었습니다. 

게다가 LDA는 막대한 양의 데이터셋에서 계산되는 비용이 너무 큽니다.



[18]과 비슷하게, 다른 모델 구조들과 비교하기 위해서 연구진은 첫 컴퓨터적으로 복잡한 모델을 모델이 학습하는데 필요한 파라미터의 개수로써 정의하였습니다.

다음으로 연구진은 정확도를 높이려고 노력할 것이며, 컴퓨터적 복잡도는 최소화시킬 것 입니다.

앞으로 사용할 모델들을 비교하기 위해서, 학습 복잡도의 비율은 다음과 같습니다.

$O = E \times T \times Q$

- $E$ 는 학습 epochs 횟수
- $T$ 는 학습 데이터셋의 단어의 개수
- $Q$​ 는 앞으로 사용될 모델 구조

일반적으로 epochs은 3-50이고 단어의 개수는 수십억개가 사용되며 모든 모델들은 경사하강법과 역전파를 사용하여 학습되어집니다.

#### 3줄 요약

- LSA, LDA보다 신경망 네트워크를 통한 학습이 훨씬 비용도 덜들고 정확하다.

- 다른 모델들의 구조들의 성능을 비교하기 위해서 비교 식을 정의합니다.

  $O = E \times T \times Q$

  - $E$ 는 학습 epochs 횟수
  - $T$ 는 학습 데이터셋의 단어의 개수
  - $Q$​ 는 앞으로 사용될 모델 구조





### 2.1 Feedforward Neural Net Language Model (NNLM)

확률적 순방향 신경망 네트워크 언어 모델은 [1]에서 제안되어져 왔습니다. 해당 모델은 입력층, 사영층, 은닉층, 출력층으로 구성되어 잇습니다.

$N$개의 단어들은 입력층에선, $V$가 단어장의 크기일 때, $N$ 단어들을 1-of $V$ 로 인코딩 시켜주었습니다.

입력층은 사영층인 $P$​로 사영되어지고 나서 차원은 $N \times D$​​ 의 크기를 가집니다. 

입력층이 $N$​ 개 만큼 활성화되기 때문에 사영층은 상대적으로 값싼 연산입니다.



NNLM 구조들은 사영층과 은닉층 사이의 연산은 사영층이 밀집되어지면서 복잡해졌습니다.

단어들이 10개 선택되었을 때, 사영층 $P$의 크기는 500에서 2000사이 일것 입니다, 반면에 은닉층인 $H$의 크기는 500에서 1000개의 유닛들을 가집니다. 

게다가 은닉층은 단어장 안에있는 모든 단어들의 확률 분포를 계산하는 데 사용되어집니다, 결과적으로 출력층의 차원은 $V$ 입니다. 그러므로 예제를 들어서 계산된 복잡도는 아래와 같습니다.

$Q = N \times D + N \times D \times H + H \times V$



결과적인 부분은 $H \times V$ 입니다. 하지만, 몇몇의 현실적인 해결법들은 계층적인 버전의 소프트맥스를 사용한 [25,23,18] 논문이든지, 완전히 정규화된 모델들이 학습동안에는 정규화되지 않는 모델들을 사용함을 회피하기 위해서 제안되었습니다.

단어장의 이진트리 표현과 함께, 출력되는 유닛의 개수들은 $\log_2(V)$​ 로 평가되어질 필요가 있습니다.​

그러므로 가장 계산 연산량이 큰 부분은 $N \times D \times H$ 입니다.



연구진의 모델들은 단어장이 Huffman 이진 트리로써 표현되어 질 때 계층적인 소프트맥스를 사용합니다.

이 부분은 단어들의 빈도가 신경망 네트워크 언어 모델에서 범주를 잘 획득하는데 작동하는 것이 이전의 관측을 따릅니다.

Huffman 이진 트리는 짧은 이진 코드를 단어의 빈도로 할당시킵니다, 그리고 이것은 훗날 출력에 있는 평가되어질 필요가 있는 유닛의 개수에 따라서 감소합니다.

반면에 균형잡힌 이진 트리는 평가되어지기 위해서 $\log_2(V)$​ 만큼의 출력층이 필요합니다.

Huffman 트리 기반의 계층적 소프트맥스는 $log_2 (U nigram_perplexity(V ))$​​ 만큼 필요합니다.​

예를 들어 단어장의 크기가 100만개의 단어들일 때, 이것의 결과는 약 2배의 평가속도를 보여줍니다.

반면에 이것은 신경망 네트워크 언어모델의 계산 병목현상 부분인 $N \times D \times H$​ 부분에서 눈에 띌만한 평가속도가 아닙니다, 연구진은 추후에 은닉층을 가지지 않고 굉장히 효율성에 종속적인 소프트 맥스 정규화 구조를 제안할 것 입니다.

#### 3줄요약

- NNLM 모델은 입력층, 사영층, 은닉층, 출력층으로 구성되어있으며 $Q = N \times D + N \times D \times H + H \times V$​ 만큼의 복잡도를 가집니다.
- $N \times D \times H$ 부분이 계산량이 가장 많습니다.
- 연구진의 모델에서 Huffman 트리 기반의 계층적 소프트맥스를 사용하겠습니다.



### 2.2 Recurrent Neural Net Language Model (RNNLM)

RNN 기반의 언어 모델은 순방향 NNLM 모델의 한계를 극복하는데 제안되어져 왔습니다. 예를 들어 문장의 길이 (모델 N의 순서대로) 가 정해질 필요가 있는 경우와 이론적으로 RNN이 얕은 신경망 네트워크보다 복잡한 패턴의 효율적으로 표현할 수 있습니다.



RNN 모델은 사영층을 가지고 있지 않습니다. 오직 입력, 은닉, 출력층을 가지고 있습니다.

이 모델 타입의 특별한 것은 재귀형 행렬로 은닉층 자기자신과 연결되어 있습니다, 시간 지연 연결성을 사용하여. 

이러한 연결성은 몇 종류의 단기 메모리 형태를 재귀 모델에서 형성시켜줍니다. 

왜냐하면 과거의 정보들은  현재 입력과 이전 타임 스텝의 은닉층을 기반으로 갱신되어진 은닉층의 state가 표현되어집니다.

RNN 모델의 학습때 마다 복잡도는 아래와 같습니다.

$Q = H \times H + H \times V $​

단어의 표현들인 $D$ 는 은닉층 $H$ 와 동일한 차원을 가집니다. 게다가, $H \times V$ 수식은 계층적 소프트맥스를 사용함으로써 $H \times \log_2(V)$ 로 줄어듭니다.

가장 복잡도가 높은 부분은 $H \times H$ 입니다.

#### 3줄 요약

- RNNLM이 NNLM보다 복잡한 패턴을 학습하고 단기 기억을 가지기에 더 좋습니다.
- RNNLM의 훈련 복잡도는 $Q = H \times H + H \times V $ ​입니다.



### 2.3 Parallel Training of Neural Networks

거대한 데이터셋으로 모델을 훈련시키기 위해서, 연구진은 DistBelief 라는 최고로 커다란 분산형 프레임 워크로 몇가지 모델들을 실행해봤습니다.

순방향 NNLM을 포함하여 해당 논문에서 제안된 새로운 모델들까지.

프레임 워크는 같은 모델의 다양한 복제본을 병렬적으로 실행하도록 해주었습니다. 그리고 복제본의 그래디언트 업데이트를 모든 파라미터들을 유지시킨 중앙화된 서버에 병합하였습니다.

이러한 병렬화 훈련을 위해서 연구진은 비동기적 미니배치 경사하강법을 Adagrad와 함께 사용하였습니다.

이러한 프레임 워크아래에서, 100개 이상의 모델 복제본을 사용하였고, 각각의 모델은 데이터 센터에 있는 서로 다른 기계의 많은 CPU 코어를 사용하였습니다.

#### 3줄 요약

- DistBelief 라는 분산형 프레임 워크로 다양한 모델들을 복제하여 훈련시켰습니다.
- 병렬 훈련을 하면서 비동기적 미니 배치 경사하강법과 Adagrad를 사용하였습니다.



## 3. New Log-linear Models

이번 섹션에서는, 연구진은 컴퓨터적 복잡도를 최소화하는 단어들의 분산 표현들을 학습시키기 위한 2가지 새로운 모델 구조를 제안합니다.

이전 섹션의 주요한 쟁점은 가장 높은 복잡도들이 모델에 있는 비선형 은닉층에의해서 발생한다는 것 입니다.

반면에 이러한 점은 신경망 네트워크를 더 매력적으로 만들었고, 연구진은 간단한 모델들을 연구하기로 결정하였습니다. 신경망 네트워크만큼 정확하게 데이터를 표현하지 못할지라도 데이터들을 더욱 더 효율적으로 학습시킬 수 있습니다.

새로운 구조들은 앞서 연구된 주제인 [13, 14]와 직접적인 연관이 있습니다, 신경망 네트워크 언어모델이 2가지 스텝으로 성공적으로 학습이 가능하다는 것을 발견하였습니다.: 첫번째는 연속적인 단어 벡터들은 간단한 모델을 사용하여 학습되어집니다. N-gram NNLM은 가장 높은 층에서 단어의 분산 표현들을 훈련시킵니다.

단어 벡터들을 학습시키는데 상당한 양의 작업이 집중되어 졌고, 연구진은 가장 간단한 한 가지가 되기 위해서 [13] 에서 제안된 접근법을 고려했습니다.

### 3줄 요약

- 단어들의 분산표현을 학습하고 컴퓨터 자원의 비용을 최소화 하기 위해서 2가지 모델 구조를 소개합니다.
- 효율적이고 간단한 모델을 사용하겠습니다.



### 3.1 Continuous Bag-of-Words Model

![image](https://user-images.githubusercontent.com/51338268/145683002-5d6a728b-d27b-4687-ab6b-0b3bdd7210ed.png)

첫 번째로 제안하는 구조는 순방향 NNLM과 유사한 구조로, 비선형 은닉층을 제거하고 모든 단어들의 사영층(단순한 행렬곱)이 공유되어집니다.(단지 사영 행렬 뿐만 아니라); 게다가 모든 단어들은 동일한 위치로 사영되어 집니다.

연구진은 이 구조를 bag-of-words 모델은 과거 단어들의 순서가 사영에 영향을 주지 않습니다.

게다가, 연구진은 또한 미래의 단어들을 사용합니다; 연구진은 태스크의 가장 좋은 성능을 다음 섹션에서 언급될 로그 선형 분류기가 입력을 들어오는 단어의 4번째 미래 4번째 까지 과거의 단어들을 사용한 경우에서 얻었습니다.

훈련 기준은 현재 중심 단어를 정확하게 분류하는 경우로 훈련 복잡도는

$Q = N \times D + D \times \log_2(V)$

이 모델을 CBOW로 부르겠습니다, 이는 기존의 bag-of-words 모델과 다르며, 이 모델은 문장의 연속적인 분산 표현을 사용합니다.

모델 구조는 Figure 1과 같습니다.

입력과 사영층 사이의 가중치 행렬은 모든 단어들의 위치들을  NNLM과 동일한 방식으로 공유되어 집니다.

#### 3줄 요약

- 컴퓨터 자원을 최소화시키며 단어들의 분산 표현을 학습시키는 모델구조로 CBOW 구조를 소개합니다.
- 가장 성능이 좋은 순간의 CBOW 중심 단어의 앞뒤 4개의 단어를 사용하여 중심 단어를 선형 분류기로 결과를 반환합니다.
- 해당 모델 구조의 훈련 복잡도는 $Q = N \times D + D \times \log_2(V)$ 입니다.

### 3.2 Continuous Skip-gram Model

![image](https://user-images.githubusercontent.com/51338268/145683011-77fbcc6b-8328-45be-9dc8-d00a816512ac.png)

두 번째 구조는 CBOW와 비슷하지만  대신에 문장을 통하여 현재 단어를 예측합니다.

동일한 문장안의 단어들을 기반으로 단어들의 분류를 최대화 시키려고 합니다.

좀더 정확하게는, 연구진은 각 현재의 단어들은 연속적인 사영층에 있는 로그-선형 분류기의 입력으로 들어가고 현재 단어의 이전과 이후 범위에 있는 단어들을 예측을 합니다.

연구진은 범위가 증가하면 결과 단어 벡터의 품질이 향상하는 것을 발견했습니다. 

하지만 또한 컴퓨터적 복잡도는 증가하게 됩니다.

왜냐하면 더 먼 거리의 단어들은 일반적으로 현재의 단어와 더 가까이 있는 단어들 보다 연관성이 더 작기 때문입니다.

더 멀리 있는 단어들은 더 적게 추출함으로써 가중치는 적게 줍니다.

이러한 구조의 훈련 복잡도는 다음과 같습니다.

$Q = C \times (D + D \times \log_{2} (V))$​​

$C$ 가 가장 멀리 있는 단어들입니다. 그러므로 만약 $C = 5$ 라고 할 때, 각각의 훈련하는 단어들은 무작위 번호 $R$ 을 범위 1과 5사이에서 선택합니다 그리고 $R$​ 번 단어들은 과거와 현재의 단어들을 사용합니다.

그러므로 $R \times 2$ 만큼 단어 분류가 입력되는 단어와 함께 요구되어질 것 입니다.

연구진의 실험에서는 $C = 10$ 으로 주었습니다.

#### 3줄 요약

- 입력으로 하나의 단어가 들어가고 입력된 현재의 단어의 과거 단어들과 현재 단어들을 예측합니다.
- 훈련 복잡도는 $Q = C \times (D + D \times \log_{2} (V))$ 입니다.
- 이후의 실험에서는 10번째 까지 떨어진 단어들을 예측하도록 하였습니다.



## 4. Results

![image](https://user-images.githubusercontent.com/51338268/145683706-fc423a99-1d72-475f-b94b-8f824a3da0d8.png)

#### 3줄 요약

- 단어들의 분산 표현 뿐만 아니라 단어들간의 유사성을 학습하게 되었습니다. 

  ex) big-bigger, small-smaller

- 단어들의 분산 표현으로 선형 연산또한 가능해졌습니다.

  ex) vector('biggest') - vector('big') + vector('small') = vector('smallest')

- 이러한 고차원의 단어 벡터들이 가지는 의미 관계들은 NLP 분야의 성능을 향상시키는데 사용되어 집니다.

  ex) 기계번역, 정보검색, 질의응답 시스템, etc...



### 4.1 Task Description

#### 3줄 요약

- 완성된 단어들의 분산 표현이 가지는 의미 관계성의 품질을 측정하기 을 위해 table1과 같은 의미 질문과 구문 질문 쌍들을 만들었습니다.
- 의미 질문은 8869개 구문 질문은 10675개를 만들었습니다.
- 해당 질문 쌍을 토대로 모델은 정확한 정답을 뱉었지만 **동의어의 경우 실수를 하였습니다.**



### 4.2 Maximization of Accuracy

![image](https://user-images.githubusercontent.com/51338268/145684084-abf4ba37-57c5-4344-bea4-f2c87d9a4a87.png)

#### 3줄 요약

- 단어들의 분산 표현의 차원을 늘릴수록, 학습되는 데이터의 양이 클 수록 성능이 더 좋아지는 것을 볼 수가 있었습니다.
- epochs 은 3번을 주었고 역전파와 확률적 경사 하강법을 사용하였습니다. 그리고 학습률은 0.025로 주어서 마지막 epoch에서 0에 근접하게 되었습니다.



### 4.3 Comparison of Model Architectures

![image](https://user-images.githubusercontent.com/51338268/145684537-d30ae193-303c-4d9b-90b6-598d7242b25d.png)

#### 3줄 요약

- 동일한 훈련 데이터와 640으로 동일한 크기의 단어 벡터를 사용하여 모델들의 성능을 비교하였습니다.
- 모델의 태스크는 의미적-구문적 단어 관계 예측 태스크로 단어들 간의 의미적 유사성에 집중했습니다.
- 성능은 RNNLM < NNLM < CBOW < Skip-gram 입니다. 



### 4.4 Large Scale Parallel Training of Models

![image](https://user-images.githubusercontent.com/51338268/145685671-ac5c2147-fa03-4d35-9c1f-525c436b78b7.png)



### 4.5 Microsoft Research Sentence Completion Challenge

![image](https://user-images.githubusercontent.com/51338268/145684687-6cbc5cfb-2a10-4cc4-9e16-2689f430aee9.png)

#### 3줄 요약

- 해당 챌린지는 마이크로소프트에서 개최한 대회로 1040개의 문장들을 가지고 각 문장들에 비어있는 단어들을 예측하는 태스크의 대회였습니다.



## 5. Examples of the Learned Relationships

![image](https://user-images.githubusercontent.com/51338268/145684687-6cbc5cfb-2a10-4cc4-9e16-2689f430aee9.png)



## 6. Conclusion

#### 3줄 요약

- 해당 논문에서는 컴퓨터 자원을 최소화 하는 방식으로 단어들의 분산 표현을 학습시키는 모델 구조를 CBOW, Skip-gram을 소개하고, 다른 RNNLM, NNLM 모델등과 성능을 비교하였습니다.
- 해당 논문의 연구들은 다양한 NLP 어플리케이션에 성능의 향상을 불러올 것 입니다. 특히 기계번역 분야에서 말입니다.



## 7. Follow-Up Work

#### 3줄 요약

- 해당 논문의 첫 번째 버전이 쓰여지고 싱글-머신 멀티-쓰레드기반의 C++ 코드로 CBOW와 Skip-gram 모델을 사용한 단어 벡터 계산 코드를 출시했습니다.
