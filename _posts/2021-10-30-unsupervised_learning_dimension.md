---
title: 비지도 학습 (2) 차원축소
categories: [deeplearning]
comments: true
use_math : true
---



지난 시간에는 비지도 학습으로 해결할 수 있는 문제중에서 군집화 문제에 대해서 알아보았습니다. 

이번 포스팅에서는 또 다른 문제인 차원축소 문제를 해결할 수 있는 비지도 학습 알고리즘 중에서 대표적으로 쓰이는 PCA와 T-SNE에 대해서 알아보도록 하겠습니다.



## 비지도 학습 - 차원축소

차원축소 알고리즘은 Dimesionality reduction이라고도 불리는 기법으로 말 그대로 고차원의 데이터를 저차원의 데이터로 정보의 손실을 최소화하는 방향으로 바꿔주는 방법을 말합니다.

차원축소법을 사용하는 이유는 대표적으로 2가지가 있습니다.

1. 해결하려는 문제에서 요구하는 차원의 수에 비해서 실제 차원의 수가 너무 많아 문제를 해결하는데 방해가 되는 경우가 발생하였을 때 차원축소법이 적용되어집니다.

2. 데이터를 나타내는 여러 특성(feature)들 중에서 어떤 특징이 가장 데이터를 잘 표현해주는지 알게 해주는 특성 추출(feature extraction)의 용도로도 사용됩니다.



### 1.1 PCA의 개념과 작동 과정

PCA는 Principal component analysis의 줄임말로 주성분 분석이라고도 불리며 적용되는 과정을 설명하면 아래와 같습니다.

1. PCA는 데이터 분포의 주성분을 찾아주는 방법으로 여기서 주성분이라는 의미는 데이터의 분산이 가장 큰 방향벡터를 말합니다.

2. PCA는 데이터들의 분산을 최대로 보존하면서 서로 직교(orthogonal)하는 기저(basis, 분산이 큰 방향벡터의 축)들을 찾아 고차원 공간을 저차원 공간으로 사영(project)시켜줍니다.

위의 내용을 하나씩 분석해보겠습니다.

분산이 크다는 의미는 분산의 식이 편차제곱합으로 $V(X) = E[(X-\bar{X})^2]$​​​인데, 관측값과 평균의 차이인 편차가 커지면 분산이 커지고 분산이 커지면 평균과의 거리가 멀리 있고, 데이터 간의 거리도 멀리 있다는 퍼져있다는 의미를 가진다고 볼 수 있습니다.

![image](https://i.stack.imgur.com/Q7HIP.gif)

이 말은 데이터의 분산이 큰 방향벡터는 정사영된 데이터가 가장 크게 퍼져있는 방향 벡터를 찾는다고 볼 수 있습니다.

첫 기저를 분산이 가장 클 때의 축으로 잡았을 때 해당 축은 PC축이라고 불리며 첫 번째 주성분이 됩니다.

![image](https://user-images.githubusercontent.com/51338268/139525209-74c86a7f-dca8-4557-98d9-f92437dca6fc.png)

다음 PC축을 찾을 때는 기존의 기저와 직교하는 축들 중에서 마찬가지로 분산이 가장 크게 가지는 축을 찾으며 두 번째 주성분을 가지는 또 다른 PC축이 됩니다.

이러한 과정을 거쳐서 PCA 기법은 고차원의 데이터를 저차원의 데이터로 차원축소를 하게 됩니다.



### 1.2 PCA의 특성 추출(Feature extraction)

앞서 PCA는 데이터를 나타내는 여러 특성(feature)들 중에서 어떤 특징이 가장 데이터를 잘 표현해주는지 알게 해주는 특성 추출(feature extraction)의 용도로도 사용된다고 언급하였씁니다.

기존에는 변수의 중요도에 따라서 특성을 선택하는 방식이었지만 PCA가 특성 추출이라고 불리는 이유는 PCA는 기존 특성중에서 중요한 것을 선택하는 방식이 아닌 기존의 특성들을 선형결합하는 방식을 사용하기 때문입니다.



### 2. T-SNE

일반적으로 데이터셋은 2개에서 3개 이상의 특성, 변수, 차원을 가지며 3차원 이상의 데이터는 시각화를 하는데 어려움이 발생합니다.

이러한 문제를 해결하는 방법으로 차원축소기법이 사용되는데 그 중에서도 T-SNE는 시각화에 자주 쓰이는 차원축소 알고리즘입니다. 

PCA도 물론 차원축소기법으로써 시각화가 가능하지만 데이터의 분포가 선형성을 가질 때 더 명확한 시각화가 가능하다는 단점이 있습니다. 실제의 데이터들의 분포가 항상 선형성을 가진다고 보장할 수 없기 때문에 다른 방법을 사용해야 하는 경우가 많습니다.

또한, PCA는 분산이 가장 큰 기저를 바탕으로 물리적 정보량을 유지하는 방향인데, T-SNE는 데이터들 간의 상대적 거리를 보존하는 방향으로 기존 차원의 공간에서 가까운 점들은 차원 축소된 공간에서도 여전히 가깝게 유지되는 것을 목표로 합니다.



### SNE(Stochastic Neighbor Embedding)

SNE는 고차원의 원공간에 존재하는 데이터 x의 이웃 간의 거리를 최대한 보존하는 저차원의 y를 학습하는 방법론입니다.

여기서 stochastic은 거리 정보를 확률적으로 나타내겠다는 의미로 식으로 표현하면 아래와 같습니다.

![image](https://user-images.githubusercontent.com/51338268/139527275-77252112-507f-4386-adfb-67b3a94ab152.png)

첫번째 식 p는 고차원의 원공간에서 i번째 데이터가 주어졌을 때 j번째 데이터를 선택할 확률이고 두 번째 식 q는 저차원의 임베딩된 공간에서 i번째 데이터가 j번째 데이터를 선택할 확률입니다.

SNE는 고차원의 분포 p와 저차원의 분포 q간의 차이를 최소화 하는 방향으로 학습을 진행하며 학습을 진행할 때, 학습이 제대로 이루어지는지 판단하는 지표로 Kullback-Leibler divergence를 사용하는데, 이 비용함수를 최소화하는 방향으로 학습을 진행합니다. 

이 지표는 두 분포가 완전히 다르면 1에 가까워지고 두 분포가 동일해지면 0에 가까워집니다.

학습에 사용되는 최적화 기법은 일반적으로 경사하강법(gradient descent)를 사용하여 저차원의 데이터 y들을 비용함수가 최소화 하는 방향으로 여러번 업데이트를 시켜줍니다.

원래 SNE가 전제하는 확률분포는 가우시안 분포였으나 가우시안 분포는 꼬리가 두텁지 않아 i번째 데이터에서 적당히 떨어져 있는 이웃 j번째 데이터와 아주 멀리 떨어져있는 k번째 이웃이 선택될 확률이 크게 차이나지 않은 crowding problem이 발생하였습니다.

이 문제를 해결하기 위해서 가우시안 분포보다 꼬리가 두터운 t분포를 사용하기 때문에 흔히 부르는 t-SNE가 되었습니다.



## 참고사이트

- [분산의 의미](https://mathbitsnotebook.com/Algebra1/StatisticsData/STSD.html)

- [정사영 그림 gif](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)

- [PCA 수학적 정리](https://ratsgo.github.io/machine%20learning/2017/04/24/PCA/)
- [ratsgo t-SNE](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/)

