I",<h2 id="1-lsa">1. LSA</h2>

<ul>
  <li>
    <p>LSA는 Latent Semantic Analysis의 줄임말로 DTM, TF-IDF 행렬의 잠재된 의미를 분석하는 잠재의미분석이라고도 부릅니다.</p>
  </li>
  <li>
    <p>LSA는 기존의 문서단어행렬인 DTM과 같은 빈도 기반의 벡터화가 아닌 특이값 분해를 통하여 <strong>단어와 단어, 문서와 문서, 단어와 문서들 사이의 의미적 유사성</strong>을 찾을 수 있습니다.</p>
  </li>
  <li>
    <p>DTM 행렬인 $m \times n$​ 행렬  $A$​의 특이값 분해</p>

    <ul>
      <li>
        <p>특이값 분해는 특이벡터 행렬과 특이값의 대각행렬로 분해해주는 과정을 말합니다.</p>
      </li>
      <li>
        <p>$A = U\sum V^T$​</p>

        <p><a href="https://wikidocs.net/24949"><img src="https://user-images.githubusercontent.com/51338268/145351468-4bf031a1-005a-4c25-af14-76fe4f52fd0b.png" alt="image" /></a></p>

        <ul>
          <li>
            <p>$U_t$​​ 는 문서들과 관련된 의미들을 표현한 $m \times t$​ 문서 벡터입니다.</p>
          </li>
          <li>
            <p>$V^T_t$​ 는 단어들과 관련된 의미를 표현한 $t \times n$​​ 단어 벡터입니다.</p>

            <p><strong>행렬 $t$​ 열은 전체 문서로부터 얻어낸 $t$​ 개의 주요 토픽(주제)라고 할 수 있습니다.</strong></p>
          </li>
          <li>
            <p>$\sum_t$ 은 각 의미의 중요도를 표현한 $t \times t$​ 행렬입니다.</p>

            <p>해당 부분이 중요도를 의미하는 이유는 특이값을 가진 대각 행렬이라서, 특이값은 고유값의 제곱근이고, 고유값은 고유벡터의 크기나 길이를 나타냅니다.</p>

            <p>고유벡터는 선형변환이된 공간을 나타내는 축의 역할을 하기에 선형변환된 공간의 특성을 나타내는 중요한 요소로써 새로운 공간에서의 고유벡터의 크기 즉, 고유값이 크면 해당 고유벡터의 가치는 높다고 판단할 수 있습니다.</p>

            <p>결과적으로 특이값이 크면 해당 고유값이 크다는 의미가 고유값이 크면 해당 고유벡터는 가치있는 의미를 가졌다고 볼 수 있습니다.</p>
          </li>
        </ul>
      </li>
      <li>특잇값중에서 하이퍼파라미터인 $t$개만 남기고 대응하는 특이벡터들로 행렬 $A$로 근사시키면 truncated SVD 라고 부릅니다.</li>
      <li>$t$​ 는 토픽의 개수를 의미하기도 합니다.</li>
      <li>DTM을 차원축소시켜 축소된 차언에서 근접 단어들을 토픽으로 묶습니다.
        <ul>
          <li>$t$​ 를 줄인 경우
            <ol>
              <li>의미의 중요도가 낮은 순으로 제거하기 때문에 설명력이 높은 정보만 남길 수 있게 됩니다.</li>
              <li>행렬의 의미를 방해하는 노이즈가 줄어들게 됩니다.</li>
              <li>차원의 크기가 줄어들기 때문에 계산 비용이 줄어들게 됩니다.</li>
              <li>줄일수록 값의 손실이 발생하며 손실된 정보는 다시 복구할 수 없습니다.</li>
            </ol>
          </li>
          <li>$t$ 를 크게 잡으면
            <ol>
              <li>손실되는 정보가 줄어들기 때문에 더 많은 의미와 정보를 얻을 수 있습니다.</li>
            </ol>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>LSA의 장점</strong></p>

    <ul>
      <li>특이값 분해를 통하여 쉽게 의미가 담긴 단어 벡터와 문서 벡터를 얻을 수 있습니다.</li>
    </ul>
  </li>
  <li>
    <p><strong>LSA의 단점</strong></p>

    <ul>
      <li>이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야하기 때문에 추가 유지비용이 발생하게 됩니다.</li>
    </ul>
  </li>
</ul>

<h2 id="2-lda">2. LDA</h2>

<ul>
  <li>LDA는 Latent Dirichlet Allocation의 줄임말로 통계기반 벡터화 기법으로 단어에 담긴 토픽이란는 잠재정보를 추정하는 확률 모형입니다.
    <ul>
      <li>Latent가 붙은 이유는 말뭉치 이면에 존재하는 정보들을 추론할 수 있기 때문입니다.</li>
      <li>Dirichlet인 이유는 하이퍼파라미터의 분포가 Dirichlet 분포를 따른다는 가정을 취하기 때문입니다.</li>
    </ul>
  </li>
  <li>LDA는 주어진 문서에 대하여 각 문서에 어떤 토픽들이 존재하는지, 토픽들은 어떤 단어들을 포함하는지에 대한 확률 모형으로 각 <strong>토픽의 단어 분포</strong>와 각 <strong>문서의 토픽 분포</strong>를 추정합니다.</li>
</ul>

<h3 id="21-lda에서-단어를-생성하는-과정">2.1 LDA에서 단어를 생성하는 과정</h3>

<p><a href="https://rokrokss.com/post/2018/09/26/lda-%EC%9E%A0%EC%9E%AC-%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88-%ED%95%A0%EB%8B%B9-latent-dirichlet-allocation-%EC%84%A4%EB%AA%85.html"><img src="https://user-images.githubusercontent.com/51338268/145539495-8aaa150c-0e5b-477e-b45a-f54a6f08242b.png" alt="image" /></a></p>

<p><img src="https://user-images.githubusercontent.com/51338268/145594616-96f23c93-5916-4730-a2a8-820b7ace5024.png" alt="image" /></p>

<p>동그라미는 변수를 의미하고, 화살표가 시작하는 부분은 조건을 말하며 화살표가 끝나는 지점은 결과를 의미합니다.</p>

<p>지금 부터 ‘나는 강아지 고양이 좋다’ 라는 문서가 어떻게 생성이 되는지 알아보겠습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145607925-c6b14542-832e-4105-a1ca-5316bd338d47.png" alt="image" /></p>

<p>하이퍼파라미터인 $\alpha, \beta$​​​​​ 는 Dirichlet 분포를 따릅니다.</p>

<p>변수인 $\theta_d$​​​​​​​​ 는 $\alpha$​​​​​​​ 영향을 받기 때문에 $\prod_{d=i}^Dp(\theta_d \vert \alpha)$​​​​​​​​​ 로 표현이 됩니다.</p>

<p>변수인 $\phi_k$​ 는 $\beta$​ 영향을 받기 때문에 $\prod_{k=i}^Kp(\phi_d \vert \beta)$​ 로 표현이 됩니다.</p>

<p>토픽의 개수는 사용자가 정할 수 있는 하이퍼파라미터이고 각 토픽들은 단어 분포를 가지고 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145595489-3b36384d-c77e-47d1-9bfa-b57ca8e68f9f.png" alt="image" /></p>

<p>각 문서들은 토픽 분포를 가지고 있으며 여러가지 토픽들이 차지하는 비율이라고 이해할 수 있습니다.</p>

<p>변수인 $z_{d,n}$​​​​​​​ 가  $\theta_{d}$​​​​​​​ 에 영향을 받기 때문에 $\prod_{n=1}^Np(z_{d,n} \vert \theta_{d})$​​​​​​​ 로 표현이 됩니다.</p>

<p>지금부터는 문서1에서 단어가 생성되는 과정을 알아보겠습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145596127-1f2c9203-2cca-43da-8999-3e2a2528a5bb.png" alt="image" /></p>

<p>문서1의 경우 토픽을 확률적으로 추출하게 되면 토픽1의 확률이 0.7로 가장 높기 때문에 토픽1에서 나온 단어들이 많을 확률이 높습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145596273-205faefe-18bd-46d6-af80-61e43be78d53.png" alt="image" /></p>

<p>지금 과정의 경우에도 문서1에서 확률적으로 가장 높은 토픽1을 선택하겠습니다.</p>

<p>$w_{d,n}$​는$z_{d,n}$​과 $\phi_{k}$에 동시에 영향 받기 때문에 ​ $w_{d,n} = \prod ^{N}<em>{n=1} p(w</em>{d,n} \vert z_{d,n}, \phi_k)$ 로 표현이 됩니다.​​​​</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145596848-b2c19734-d691-47b7-a87f-7f4491825332.png" alt="image" /></p>

<p>토픽1의 단어분포는 “나는”이 0.75로 단어로 생성될 확률이 가장 높고 “고양이”는 0.01로 단어로 생성될 확률이 가장 낮습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145598640-f2a9086c-1f23-4b61-85a6-cfb122cb732e.png" alt="image" /></p>

<p>토픽1에서도 단어 분포에 따라서 많은 단어들을 확률적으로 선택하게 되며 지금 같은 경우에는 “나는” 이라는 단어가 0.75로 가장 높기 때문에 생성될 가능성이 높습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/145599038-e87bc8b4-0edb-45fe-a4ec-781cd18f20a3.png" alt="image" /></p>

<p>최종적으로 LDA 확률 모형이 단어를 생성하는 과정에 따라서 문서1이 가진 토픽 분포에서 토픽1을 뽑혔고, 토픽1이 가진 단어 분포에서 “나는”이라는 단어가 생성되었습니다.</p>

<p>이 과정을 반복하면 토픽의 단어 분포에서 단어가 지속적으로 생성되어 “나는 고양이 강아지 좋다” 라는 문서를 완성하게 되며 수식으로 표현하면 다음과 같습니다.</p>

<p>$p(\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:d}) = \prod_{d=i}^Dp(\theta_d \vert \alpha) \prod_{k=i}^Kp(\phi_d \vert \beta) \prod_{n=1}^Np(z_{d,n} \vert \theta_{d})p(w_{d,n} \vert z_{d,n}, \phi_k)$</p>

<p>토픽의 단어 분포와 문서의 토픽 분포에 대한 결합확률이 큰 방향으로 합니다.</p>

<p>$\arg \max_{z, \phi, \theta} p(z, \phi, \theta \vert w) = \frac{p(z, \phi, \theta, w)}{p(w)}$​</p>

<p>$p(w)$​​ 를 구해야 하지만 실제 값을 구하기는 어렵기 때문에 <strong>깁스 샘플링</strong>을 통하여 $p(w)$​​ 에 근사하고 토픽을 추론하게 됩니다.</p>

<h3 id="22-lda와-깁스-샘플링">2.2 LDA와 깁스 샘플링</h3>

<p>$d$​ 번째 문서 $n$​ 번째 단어가 실제 $j$ 번째 토픽이 될 확률을 깁스 샘플링을 적용한 수식을 다음과 같습니다.</p>

<p>$p(z_{d,i} = j \vert z_{-i}, w) = \frac{n_{d,k} + \alpha_j}{\sum^K_{i=1}(n_{d,i} + \alpha_i)} \times \frac{v_{k,w_{d,n}} + \beta_{w_{d,n}}}{\sum^V_{j=1}(v_{k,j} + \beta_j)} = AB$​</p>

<p>이 식에서 중요한 것은 $AB$ 입니다.</p>

<ul>
  <li>$A$ : $d$ 번째 문서가 $k$ 번째 토픽과 맺고 있는 연관성 정도</li>
  <li>$B$​ : $d$​ 번째 문서의 $n$ 번째 단어가 $k$​ 번째 토픽과 맺고 있는 연관성 정도</li>
</ul>

<p>LDA의 깁스 샘플링은 각 단어에 잠재된 토픽을 추론하는 방식으로 진행됩니다.</p>

<ol>
  <li>
    <p>문서의 모든 단어에 토픽을 무작위로 할당해준다고 가정합니다.</p>
  </li>
  <li>
    <p>모든 단어에 토픽을 부여하였기에 단어별 토픽 분포가 생기게 됩니다.</p>
  </li>
  <li>
    <p>예를 들어서 문서1의 두 번째 단어의 잠재된 토픽이 무엇인지 추론할 때, 깁스 샘플링은 문서1의 두 번째 단어의 토픽 정보를 지워줍니다.</p>
  </li>
  <li>
    <p>단어에 있는 토픽이 사라지면 단어들의 토픽 분포와 토픽의 단어 분포에 영향을 주게 되어 두 분포가 사라진 토픽을 반영하여 바뀌게 됩니다.</p>
  </li>
  <li>문서1의 두 번쨰 단어의 토픽은 $AB$ 로 구해지며 구해진 단어의 토픽 분포는 확률적인 방식으로 선택되어집니다.</li>
  <li>모든 문서, 모든 단어에 깁스 샘플링을 수행하면 모든 단어마다 토픽을 할당시켜줄 수 있게 됩니다.</li>
</ol>

:ET