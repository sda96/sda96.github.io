I"<h1 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding">[XLNet] Generalized Autoregressive Pretraining for Language Understanding</h1>

<p><a href="https://arxiv.org/abs/1906.08237"><strong>[XLNet] 언어 이해를 위한 일반화된 자기회귀 사전학습 시키기</strong></a></p>

<p>해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.</p>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>흔히 auto-encodeing 방식의 모델인 BERT가 auto-regressive 방식의 모델보다 성능이 좋습니다.</li>
  <li>하지만 BERT의 경우 마스크되어진 위치간의 종속성이 무시되어지며 사전학습, 미세조정 단계에서 여러가지 어려움을 겪습니다.</li>
  <li>이러한 문제들을 해결하기 위해서 연구진은 XLNet을 소개하며 XLNet은 다음 방법을 통하여 문제들을 극복했습니다.
    <ul>
      <li>문장의 모든 순열 조합들중에서 평균 가능도를 최대화시키는 문장을 선택하며 모든 순열 조합의 경우의 수를 학습시키기에 문장을 양방향으로 학습시키는 효과가 납니다.</li>
      <li>auto-regressive 구조를 통하여 기존의 BERT 모델의 한계를 극복합니다.</li>
      <li>사전학습 단계에서 Transformer-XL의 내용을 차용하였습니다.</li>
    </ul>
  </li>
</ul>

<h3 id="introduction">Introduction</h3>

<ul>
  <li>최근 연구들의 추세는 커다란 크기의 레이블 되지 않은 말뭉치를 뉴럴넷에 사전학습시키고, 모델이나 표현을 downstream task에 맞게 미세조정을 시켜줍니다.</li>
  <li>가장 성공적인 사전학습 구조는 AR과 AE입니다.</li>
  <li>AR은 Auto-Regressive 구조로 오직 단방향으로 인코드 된 것을 학습하기 때문에 양방향성 문장으로 효율적이게 모델링 시킬 수 없습니다.</li>
  <li>AE는 Auto-Encoding 구조로 대표적인 예시로 BERT가 존재하며 BERT는 양방향성 문장을 학습할 수 있습니다.</li>
  <li>하지만 BERT에서 사용되는 마스크 토큰은 학습 데이터와 실제 데이터 사이의 차이점을 발생시키게 만듭니다.</li>
  <li>이러한 문제점에 대하여 연구진은 XLNet을 통하여 AR과 AE의 각 장점만을 취하고 단점을 피하고자 합니다.
    <ul>
      <li>첫번째로, 기존 AR모델에 factorization order를 사용하여 양방향 학습 효과를 노립니다.</li>
      <li>두번째로, AR 모델을 일반화시킴으로써 BERT가 겪은 사전학습, 미세조정 단계의 문제점으로 고통받지 않을 수 있습니다.</li>
      <li>추가적으로 사전학습 단계에서 Transformer-XL의 segment 재귀구조, 상대적 포지셔널 인코딩 방법을 차용하겠습니다.</li>
    </ul>
  </li>
</ul>
:ET