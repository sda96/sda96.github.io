I"[<h1 id="gpt-improving-language-understanding-by-generative-pre-training">[GPT] Improving Language Understanding by Generative Pre-Training</h1>

<p><strong>[GPT] 생성가능한 사전 학습시키기를 통한 언어 이해 성능 향상시키기</strong></p>

<p>해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.</p>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>해당 연구진은 막대한 양의 정보를 태스크별로 달성하기 위해서 <strong>레이블이 되지 않은 텍스트의 다양한 말뭉치</strong>를 사용한 언어 모델을 바탕으로 <strong>‘generative pre-training’</strong> 과, 각 태스크별로 <strong>‘discriminative fine-tuning’</strong> 사용하였더난 것을 증명하였습니다.</li>
</ul>

<h3 id="introduction">Introduction</h3>

<ul>
  <li>원본 텍스트로부터 효율적이게 학습하는 능력은 NLP 분야에 있는 지도학습에 대한 의존성을 완화시키는데 중대한 사안입니다.</li>
  <li>지도학습 방식은 모델의 활용성과 성능을 저해시키는 방법입니다.</li>
  <li>레이블이 되지않은 텍스트 데이터를 학습하는 방식에서 발생하는 2가지 도전에 대한 주요한 원인이 있습니다.</li>
  <li>첫 번째로, 최적화시키려는 객체의 종류가 전이에 유용한 텍스트 표현을 학습하는데 가장 효과적인지 불확실합니다.</li>
  <li>
    <p>두 번째로, 타겟 태스크에 학습된 표현들을 전이시키는데가장 효율적인 방법에 대한 의견이 없었습니다.</p>
  </li>
  <li>
    <p>해당 논문에서는 연구진은 <strong>비지도 사전학습</strong>과 <strong>지도 미세조정</strong>법을 결합하여 언어 이해 태스크를 위한 <strong>반지도 접근법</strong>을 탐구합니다.</p>
  </li>
  <li>
    <p>연구진의 목적은 다양한 분야의 태스크에 약간의 학습을 전이시켜서 전반적인 표현을 학습시킬 수 있는 것 입니다.</p>
  </li>
  <li>연구진은 레이블 되지않은 막대한 말뭉치로 몇가지 격식화된 훈련 예제들을 추정하도록 만들 것 입니다.</li>
  <li>연구진은 <strong>two-state 학습 절차</strong>를 적용하였으며 첫 번째로 언어 모델링된 객체를 레이블이 되지 않은 데이터를 사용하는데 신경망 네트워크 모델의 초기 파라미터를 학습시키기 위해서 사용하겠습니다.</li>
  <li>연구진은 추가적으로 이러한 파라미터들을 지도된 객체와 일치하는 타겟 태스크에 적용합니다.</li>
  <li>연구진은 Transformer 구조를 사용하였으며 해당 방법이 다양한 태스크에서 좋은 성능을 보여주었습니다.</li>
</ul>
:ET