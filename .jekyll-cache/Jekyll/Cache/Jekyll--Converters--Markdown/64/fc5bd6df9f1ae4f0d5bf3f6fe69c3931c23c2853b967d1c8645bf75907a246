I"<h1 id="bahdanau-attention-neural-machine-translation-by-jointly-learning-to-align-and-translate">[Bahdanau Attention] NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</h1>

<p>해당 번역본은 정식 번역본이 아니며 개인이 공부를 위해서 번역한 내용으로 많은 오역, 오타가 존재합니다. 이러한 점을 감안해주시고 읽어주시면 감사하겠습니다.</p>

<h2 id="abstract">Abstract</h2>

<ul>
  <li>연구진은 기존의 인코더-디코더 구조에서 성능을 향상시키는데 병목현상을 일으키는 원인이 고정된 길이의 벡터를 사용하기 때문이라 생각했습니다.</li>
  <li>연구진은 <strong>예측하려는 하나의 타겟 단어와 관련있는 부분들을 입력 문장에서 (자동적으로 소프트하게) 탐색하는 모델</strong>을 제안하였습니다.</li>
  <li>해당 방법은 영어-프랑스어 번역 태스크에서 존재하는 SOTA와 비교할만한 성능을 얻었습니다.</li>
  <li>또한, 정성적인 분석을 한 결과 해당 모델이 탐색한 것들이 사람의 직관과도 잘 맞다는 것을 발견했습니다.</li>
</ul>

<h3 id="1-introduction">1. Introduction</h3>

<h4 id="11-신경망-기계번역에서-사용되는-인코더-디코더-구조">1.1 신경망 기계번역에서 사용되는 인코더-디코더 구조</h4>

<ul>
  <li>기계번역 분야에서 최근 신경망 기계번역이 떠오르고 있으며 모델의 구조는 일반적으로 인코더-디코더 구조입니다.</li>
  <li>해당 구조는 인코더가 입력 문장을 읽고 고정 길이 벡터로 인코딩하며 디코더는 인코딩된 벡터를 받아서 번역된 결과물을 출력합니다.</li>
  <li>인코더-디코더 구조는 한 쌍의 언어를 다루며 입력 문장이 주어졌을 때, 올바른 번역을 반환시킬 확률을 높이기 위해서 공동으로(인코더와 디코더가 동시에) 학습되어집니다.</li>
</ul>

<h4 id="12-기존-인코더-디코더-문제점">1.2 기존 인코더 디코더 문제점</h4>

<ul>
  <li>인코더-디코더 구조의 인코더는 입력 문장에 있는 필요한 정보들을 모두 압축시키기에 긴 문장일수록 모델의 성능이 급격히 나빠지는 문제가 발생합니다.</li>
</ul>

<h4 id="13-문제-해결-방법-제시">1.3 문제 해결 방법 제시</h4>

<ul>
  <li>이러한 문제를 해결하기 위해서 해당 논문에서는 새로운 접근법을 제안합니다.</li>
  <li>
    <p>각각의 시점에 모델이 하나의 단어를 생성할 때, 입력 문장에서 (생성된 단어와) 가장 연관된 정보를 가진 위치들의 집합을 찾는데 집중합니다.</p>
  </li>
  <li>모델이 하나의 타겟 단어를 예측할 때, <strong>입력 문장의 위치값과 관련된 문장벡터들</strong>과 <strong>이전에 생성되어진 모든 타겟 단어들</strong>을 기반으로 예측합니다.</li>
  <li>해당 논문의 접근법이 기존의 인코더 디코더 모델과 다른 특징을 가집니다.
    <ul>
      <li>기존 방법은 고정 길이 벡터로 인코딩 되었지만 논문의 접근법은 벡터들을 하나의 시퀀스로 인코딩합니다.</li>
      <li>번역으로 디코딩되는 동안 적응되는(학습되는) 벡터들의 부분집합을 선택합니다.</li>
    </ul>
  </li>
  <li>해당 접근법으로 고정 길이 벡터임에도 불구하고 입력 문장의 정보를 압착할 수 있게 되었고 긴 문장에서도 성능이 괜찮았습니다.</li>
</ul>

<h3 id="2-background-neural-machine-translation">2. BACKGROUND: NEURAL MACHINE TRANSLATION</h3>

<ul>
  <li>일반적인 기계번역 모델은 입력 문장 $\bold x$​가 주어졌을 때, 출력 문장 $\bold y$인 조건부 확률이 가장 크게되는 출력 문장 $y$를 구하는 것이 기본 구조입니다.
    <ul>
      <li>$\arg max_y p(\bold y \lvert \bold x)$​</li>
    </ul>
  </li>
  <li>신경망 기계 번역의 경우 입력문장을 인코딩 시키는 인코더와 인코딩된 입력 문장을 받아서 출력 문장을 생성하는 디코더의 구조로 되어있습니다.
    <ul>
      <li>인코더, 디코더의 구조이기에 필요한 모델의 개수도 2개입니다.</li>
      <li>모델로 RNN모델을 사용되어질 수 있습니다.</li>
    </ul>
  </li>
  <li>신경망의 번역 시스템의 방식으로 2가지가 있습니다.
    <ul>
      <li>한 쌍의 문장들로 이루어진 문장 테이블에 점수를 매기는 방식</li>
      <li>후보 번역본들의 순위를 재지정하는 방식</li>
    </ul>
  </li>
</ul>

<h3 id="3-learning-to-align-and-translate">3. LEARNING TO ALIGN AND TRANSLATE</h3>

<ul>
  <li>인코더 부분은 양방향 RNN 모델을 사용하였으며 디코더 부분은 다음과 같습니다.</li>
</ul>

<h4 id="31-디코더">3.1 디코더</h4>

<p><img src="https://user-images.githubusercontent.com/51338268/146635782-e4b40927-0d1c-47b5-a893-b94834e193b3.png" alt="image" /></p>

<ul>
  <li>$p(y_i \lvert y_1 \cdots, y_{i-1} , \bold x) = g(y_{i-1}, s_i, c_i)$
    <ul>
      <li>$y_i$는 $i$시점의 출력값</li>
      <li>$\bold x$는 입력 문장 벡터로 $x_1, x_2, \cdots x_T$ 는 $T$시점의 단어들입니다.</li>
      <li>$s_i$는 $i$시점의 RNN 모델의 hidden state</li>
      <li>$c_i$는 $i$시점의 context vector로 context vector란 <strong>인코더에서 가져온 hidden state</strong>라고 보면 됩니다.</li>
    </ul>
  </li>
  <li>$s_i = f(s_{i-1}, y_{i-1}, c_i)$</li>
  <li>$c_i = \sum^{T_x}<em>{j=1} \alpha</em>{ij}h_j$​
    <ul>
      <li>$c_i$는 $h_j$의 가중합입니다.</li>
      <li>$\alpha_{ij} = \frac{exp(e_{ij})}{\sum^{T_{x}}<em>{k=1}exp(e</em>{ik})}$​로 소프트맥스를 하기에 $\alpha_{ij}$​는 0과 1사이의 확률이기에 점수로 볼 수 있습니다.</li>
      <li>$e_{ij} = a(s_{i-1}, h_j)$​를 alignment model이라고 부릅니다.
        <ul>
          <li>$a$​는 학습이 가능한 파라미터입니다.</li>
          <li>$e_{ij}$는 입력층의 hidden state인 $h_j$의 중요도를 반영합니다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h4 id="32-인코더">3.2 인코더</h4>

<p>인코더부분은 BiRNN 구조를 사용하였습니다.</p>

<h3 id="4-experiment-settings">4. EXPERIMENT SETTINGS</h3>

<p>연구진은 ACL WMT 14 데이터셋에 있는 영어-프랑스어 번역 태스크로 성능을 실험한 결과 다음과 같습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/146674736-5cdfbc84-0816-4c71-82b5-c48c8772e1a9.png" alt="image" /></p>

<p>BLEU score 지표를 기준으로 기존 인코더 디코더 모델과 연구진의 개선된 인코더 디코더 모델의 성능을 문장의 길이에 따라서 비교하였습니다.</p>

<p>결과는 &lt;unk&gt;이 포함된 문장의 길이가 길어질수록 연구진의 개선된 인코더 디코더 모델의 성능은 안정적으로 유지되는 것을 알 수가 있었습니다.</p>
:ET