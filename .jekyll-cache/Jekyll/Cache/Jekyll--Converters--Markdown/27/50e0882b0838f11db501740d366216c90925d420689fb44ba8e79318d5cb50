I"<h2 id="1-정보량information-content">1. 정보량(Information content)</h2>

<p>정보량은 정보이론(information theory)에서 가져온 개념으로 추상적인 ‘정보’라는 개념을 정량화한 지표입니다.</p>

<p>Goodfellow, Bengio, Courville의 책 <a href="http://www.deeplearningbook.org/">Deep Learning</a>에서 정보를 정량적으로 만들기 위한 3가지 조건이 있습니다</p>

<ol>
  <li>일어날 가능성이 높은 사건은 정보량이 낮으며 반드시 일어나는 사건은 정보가 없는 것과 마찬가지 입니다.</li>
  <li>반대로 일어날 가능성이 낮은 사건은 정보량이 높습니다.</li>
  <li>두 개의 독립적인 사건이 존재할 때, 전체 정보량은 각각의 정보량을 더한 것과 동일합니다.</li>
</ol>

<p>이러한 조건속에서 사건 $x$가 일어날 확률을 $P(X=x)$라고 할 때, 사건의 정보량의 수식은 아래와 같습니다.</p>

<p>$I(x) = -\log_bP(x)$​</p>

<p>로그의 밑인 $b$는 주로 2, e, 10과 같은 값이 사용되는데 b=2인 경우 정보량은 정보를 표현하기 위해서 필요한 비트(bit)의 개수와 같습니다.</p>

<p>위 식을 그래프로 그리면 앞서 말한 3가지 조건이 충족된다는 것을 알 수가 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/141257842-6146c497-fe2c-4532-83ea-1d51706fd279.png" alt="image" /></p>

<p>그림과 같이 발생할 확률이 높은 사건의 정보량은 갈수록 감소하여 반드시 발생할 확률인 1에 도달하면 정보량은 0이 되어 전혀 가치가 없는 정보가 됩니다.</p>

<p>반대로 사건이 발생할 확률이 낮아질수록 정보량이 가지는 값은 $\infty$​에 발산하게 되며 이 해당 사건이 가지는 정보의 가치가 높다고 판단할 수 있습니다.</p>

<h2 id="2-엔트로피entropy">2. 엔트로피(Entropy)</h2>

<p>일반적으로 엔트로피는 무질서도 또는 불확실성을 의미하지만, 엔트로피의 정보이론적 관점의 정의는 특정 확률분포를 따르는 사건들의 정보량에 대한 기댓값을 의미합니다.</p>

<p>확률 변수가 이산형이면 아래와 같은 수식으로 표현할 수 있습니다.</p>

<p>$H(x) = E_{X\sim{P}}[I(x)] = -\sum^n_{i=1}p_ilog p_i$</p>

<p>$p_i := P(X=x_i)$​</p>

<p>동전 던지기를 예시로 들면 동전 던지기에서 발생할 수 있는 사건은 2가지 경우로 동전이 앞면으로 나왔을때와 동전이 뒷면으로 나오는 경우가 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/141262181-084661d4-842d-4f53-aa88-c26989bc5184.png" alt="image" /></p>

<p>이 때, 동전의 앞면이 나올 확률이 0.5인 경우 뒷면 또한 나올 확률이 0.5로 엔트로피는 가장 커지게 되며 이는 앞으로의 발생할 사건의 결과가 어떻게 나올지 모르기 때문에 불확실성이 가장 커지는 순간이라고 볼 수  있습니다.</p>

<p>하지만 만약 동전이 앞면이 나올 확률이 0.99이고 뒷면이 나올 확률이 0.01이면 앞으로 발생할 사건의 결과를 앞면이라고 쉽게 예측이 가능하기 때문에 확률이 0.5일때 보다  불확실성, 즉 엔트로피는 비교적 굉장히 작아지게 됩니다.</p>

<p>확률 변수가 연속형이면 아래와 같은 수식으로 표현할 수 있습니다.</p>

<p>$h(x) = -\int p(x)\log p(x)dx$​​​​​​​</p>

<p>연속 확률 변수의 엔트로피를 이산 확률 변수와 구분하기 위해서 미분 엔트로피(differential entropy)라고 부르기도 합니다.</p>

<h2 id="3-kullback-leibler-divergence">3. Kullback Leibler Divergence</h2>

<p>머신러닝 모델은 크게 결정 모델과 생성 모델로 구분이 가능합니다.</p>

<p>결정 모델(discriminative model)은 데이터의 실제 분포를 모델링 하지 않고, 결정 경계(deicision boundary)만을 학습합니다.</p>

<p>예를 들어 모델의 결과가 0보다 작을 경우 1번 클래스로 분류하고 0보다 크면 2번 클래스로 분류하는 방식이라고 생각하면 됩니다.</p>

<p>반면에 생성 모델은 데이터와 모델로부터 도출할 수 있는 여러가지 확률 분포들과 베이즈 이론을 활용하여 데이터의 예측값의 분포를 실제 분포에 간접적으로 모델링을 합니다.</p>

<p>생성 모델이 만든 확률 분포와 실제 확률 분포의 차이를 나타내기 위해서 쿨백 라이블러 발산(Kullback Leibler Divergence)를 지표로써 활용을 합니다. KL divergence라고도 불립니다.</p>

<p>$D_{KL}(P\vert\vert Q)=E_{X∼P}[−\log{Q(x)}]−E_{X∼P}[−\log{P(x)}] = ∑P(x)\log{\frac{P(x)}{Q(x)}}$​​​​​​​​​​​​</p>

<p>위의 식은 확률 변수가 이산형일 때이고, 아래의 식은 확률 변수가 연속형일 때입니다.</p>

<p>$D_{KL}(P\vert\vert Q) = \int P(x)\log{\frac{P(x)}{Q(x)}}dx$​​​​​​​​​​​​​​</p>

<p>실제 확률 분포 $P(x)$​​​와 예측 확률 분포 $Q(x)$​​​의 차이의 값을 알아내기 위해서 확률 $p(x)$​​​를 기준으로 실제 확률 분포의 엔트로피와 예측 확률 분포의 엔트로피를 뺀 값이라고 보면 됩니다.</p>

<h4 id="kl-divergence의-특징">KL divergence의 특징</h4>

<ul>
  <li>KL divergence는 두 확률 분포의 차이를 나타내는 값이기에 거리 함수와 비슷한 성질을 지녔지만 기준이 되는 $p(x)$​​​​가 다르므로 $D_{KL}(P\vert\vert Q)$​​​​는 $D_{KL}(Q\vert\vert P)$​​​​와 같지 않습니다.</li>
  <li>$D_{KL}(P\vert\vert Q)$​는 0보다 크거나 같습니다</li>
  <li>$D_{KL}(P\vert\vert Q) = 0$​이면 두 확률 분포는 동일합니다.</li>
</ul>

<h2 id="교차-엔트로피cross-entropy">교차 엔트로피(Cross Entropy)</h2>

<p>$D_{KL}(P\vert\vert Q)= ∑P(x)\log{\frac{P(x)}{Q(x)}} = (-∑P(x)\log{Q(x)} ) - (-∑P(x)\log{P(x)})$​​​​​</p>

<p>만약 두 확률 분포의 차이를 줄이기 위해서 KL divergence를 최소화 시킬 때, $P(x)$​는 실제 데이터의 확률 분포이기 때문에 바꿀 수 없는 고정된 값입니다. 대신에 예측 데이터의 확률 분포인 $Q(x)$​를 최소화 시켜야 하는 문제로 바꿀 수 있으며 이 때, $P(x)$​를 기준으로 계산한 $Q(x)$​의 엔트로피를 교차 엔트로피라고 부릅니다.</p>

<p>$P(x)$에 대한 $Q(x)$의 교차 엔트로피 식을 다시 적으면 다음과 같습니다.</p>

<p>$H(P,Q) = -E_{X\sim{P}}[\log{Q(x)}] = - \sum{P(x)\log{Q(x)}}$​</p>

<p>KL divergence의 계산식으로부터 엔트로피와 교차 엔트로피의 관계식을 얻을 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/51338268/141270622-62a6384c-43b9-4038-8a66-fb5cec008dc2.png" alt="image" /></p>

<p>정리하면 엔트로피와 KL divergence를 더하면 교차 엔트로피가 나온다고 볼 수 있습니다.</p>
:ET